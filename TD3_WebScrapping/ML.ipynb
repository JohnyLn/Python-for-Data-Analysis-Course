{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/getting_started.html\n",
    "5.1) Simple Classification using RandomForest algorithm\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587–588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random forests are frequently used as \"blackbox\" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration in packages such as scikit-learn.\n",
    "# Model Choice , Model Import\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model (classifier) instanciation, tuning hyper parameters\n",
    "# This algorithm need some random numbers to start\n",
    "\n",
    "# since we're giving a fixed random_state, the classifier should behave the same every time\n",
    "clf = RandomForestClassifier(random_state=0) \n",
    "\n",
    "\n",
    "# Creation of the features matrix : 1 sample / line, 1 variable/feature /column\n",
    "X = [[ 1,  2,  3], [11, 12, 13]]  # 2 samples, 3 features\n",
    "\n",
    "# The samples (i.e., rows) always refer to the individual objects described by the dataset.\n",
    "# a flower, an email, an image, a person..\n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the target array  : one label/class / line\n",
    "# The array must contains the same nb of lines as the features matrix\n",
    "y = [0, 1]  # classes of each sample\n",
    "\n",
    "# Fit the model to your data ( training)\n",
    "clf.fit(X, y)\n",
    "# now that the model is trained, we can use it to make predictions on unseen data :\n",
    "\n",
    "clf.predict(X)  # predict classes of the training data\n",
    "clf.predict([[4, 5, 12], [14, 15, 16]])\n",
    "clf.predict([[1, 7, 10]]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model de base avec sciler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-55f3f41486cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# we need to modify x to make it a matrix of size [n_samples, n_features].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#  numpy.newaxis is used to increase the dimension of the existing array by one more dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(46)\n",
    "# Model Choice , Model Import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model instanciation, tuning hyper parameters\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model\n",
    "# Model Choice , Model Import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model instanciation, tuning hyper parameters\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model\n",
    "# we need to modify x to make it a matrix of size [n_samples, n_features]. \n",
    "X = x[:, np.newaxis]\n",
    "#  numpy.newaxis is used to increase the dimension of the existing array by one more dimension\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eatures engineering ou data preparation\n",
    "model.coef_\n",
    "model.intercept_\n",
    "# now that the model is trained, we can use it to make predictions on unseen data.\n",
    "# let's generate some data : \n",
    "xfit = np.linspace(-1, 11)\n",
    "\n",
    "# as before, we need to adapt the data to give the model a 2d matrix [n_samples, n_features] : \n",
    "Xfit = xfit[:, np.newaxis]\n",
    "\n",
    "Xfit\n",
    "# let's feed the data to the model and store his predictions into a new array\n",
    "yfit = model.predict(Xfit)\n",
    "yfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simmple classification using sklearn buultin datasets, and naive Bayes moodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "type(iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(iris) # show all attributes of an object\n",
    "print(iris.DESCR)\n",
    "Class Correlation above is Pearson's correlation coefficient. It shows linear relationship between a feature and classes.\n",
    "\n",
    "https://stackoverflow.com/questions/52905164/class-correlation-and-its-effects\n",
    "\n",
    "    type(iris.data)\n",
    "iris.data.shape[0],iris.target.shape[0] # same nb of lines\n",
    "list(iris.target_names)\n",
    "list(iris.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit learns accepts array-like data :  numpy arrays, lists of numbers, pandas DataFrames with all columns numeric, numeric pandas.Series ..\n",
    "# let's use pandas dataframe to present the data\n",
    "\n",
    "import pandas as pd\n",
    "df           = pd.DataFrame(iris['data'])\n",
    "df.columns   = iris['feature_names']\n",
    "df['target'] = iris['target']\n",
    "df\n",
    "X=iris.data # X is standard name for features matrix\n",
    "y=iris.target # y is standard name for target array\n",
    "---------------------------\n",
    "# We would like to split our data into training set and testing set\n",
    "# training set will be feed to the model, with labels, for training\n",
    "\n",
    "# testing set will be used for testing, after the model have been trained\n",
    "# we will ask the model to predict labels on the testing samples, and will compare it to the testing labels\n",
    "# for this we need to split the dataset into training set and testing set\n",
    "# we could do this by hand, but scikt learn gives us a useful function to do it \n",
    "# default size of test set is 25% of original data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "Xtrain.shape, Xtest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d992af4f84d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# quelle est la proportion test set / dataset initial par défaut ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mf\"{Xtest.shape[0]/( Xtrain.shape[0] +Xtest.shape[0] )*100:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m \u001b[1;31m# 1. choose model class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# 2. instantiate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m                  \u001b[1;31m# 3. fit model to data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xtest' is not defined"
     ]
    }
   ],
   "source": [
    "# quelle est la proportion test set / dataset initial par défaut ?\n",
    "print (f\"{Xtest.shape[0]/( Xtrain.shape[0] +Xtest.shape[0] )*100:.2f}\")\n",
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "model.fit(Xtrain, ytrain)                  # 3. fit model to data\n",
    "y_model = model.predict(Xtest)             # 4. predict on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # conventional alias\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['target'] = dataset.target\n",
    "print(boston.DESCR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df.describe()\n",
    "corr=df.corr(method='kendall')\n",
    "corr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn # just a conventional alias, don't know why\n",
    "import numpy as np\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = seaborn.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "seaborn.heatmap(corr, mask=mask,cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
